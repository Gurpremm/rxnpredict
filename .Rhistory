# RMSE and R^2 plot for different machine learning models
# ============================================================================
# Predict for testing set
lm.pred <- predict(lmFit, test.scaled)
svm.pred <- predict(svmFit, test.scaled)
knn.pred <- predict(knnFit, test.scaled)
nnet.pred <- predict(nnetFit, test.scaled)
bayesglm.pred <- predict(bayesglmFit, test.scaled)
# R^2 values
lm.pred.r2 <- cor(lm.pred, test.scaled$yield)
svm.pred.r2 <- cor(svm.pred, test.scaled$yield)
knn.pred.r2 <- cor(knn.pred, test.scaled$yield)
nnet.pred.r2 <- cor(nnet.pred, test.scaled$yield)
bayesglm.pred.r2 <- cor(bayesglm.pred, test.scaled$yield)
# RMSE
lm.pred.rmse <- rmse(lm.pred, test.scaled$yield)
svm.pred.rmse <- rmse(svm.pred, test.scaled$yield)
knn.pred.rmse <- rmse(knn.pred, test.scaled$yield)
nnet.pred.rmse <- rmse(nnet.pred, test.scaled$yield)
bayesglm.pred.rmse <- rmse(bayesglm.pred, test.scaled$yield)
# Plot RMSE and R^2
df <- data.frame(rmse = c(lm.pred.rmse, svm.pred.rmse, knn.pred.rmse, nnet.pred.rmse, bayesglm.pred.rmse, rf.pred70.rmse),
r2 = c(lm.pred.r2, svm.pred.r2, knn.pred.r2, nnet.pred.r2, bayesglm.pred.r2, rf.pred70.r2))
row.names(df) <- c('Linear Model', 'SVM', 'kNN', 'Neural Network', 'Bayes GLM', 'Random Forest')
rmse.plot <- ggplot(df, aes(y=reorder(rownames(df), rmse), x=rmse)) +
geom_point() +
geom_text(label=round(df$rmse, 2), vjust=-1, size=3) +
labs(x='RMSE', y='') +
xlim(0,20)
r2.plot <- ggplot(df, aes(y=reorder(rownames(df), rmse), x=r2)) +
geom_point() +
geom_text(label=round(df$r2, 2), vjust=-1, size=3) +
labs(x='Rsquared', y='') +
xlim(0.7,1)
plots <- arrangeGrob(rmse.plot, r2.plot, ncol=2)
ggsave(plots, file="R\\plots\\model_comp_other.png", width=8, height=3)
# ============================================================================
# RMSE and R^2 plot for different machine learning models
# ============================================================================
# Predict for testing set
lm.pred <- predict(lmFit, test.scaled)
svm.pred <- predict(svmFit, test.scaled)
knn.pred <- predict(knnFit, test.scaled)
nnet.pred <- predict(nnetFit, test.scaled)
bayesglm.pred <- predict(bayesglmFit, test.scaled)
rf.pred <- predict(rfFit, test.scaled)
# R^2 values
lm.pred.r2 <- cor(lm.pred, test.scaled$yield)
svm.pred.r2 <- cor(svm.pred, test.scaled$yield)
knn.pred.r2 <- cor(knn.pred, test.scaled$yield)
nnet.pred.r2 <- cor(nnet.pred, test.scaled$yield)
bayesglm.pred.r2 <- cor(bayesglm.pred, test.scaled$yield)
rf.pred.r2 <- cor(rf.pred, test.scaled$yield)
# RMSE
lm.pred.rmse <- rmse(lm.pred, test.scaled$yield)
svm.pred.rmse <- rmse(svm.pred, test.scaled$yield)
knn.pred.rmse <- rmse(knn.pred, test.scaled$yield)
nnet.pred.rmse <- rmse(nnet.pred, test.scaled$yield)
bayesglm.pred.rmse <- rmse(bayesglm.pred, test.scaled$yield)
rf.pred.rmse <- rmse(rf.pred, test.scaled$yield)
# Plot RMSE and R^2
df <- data.frame(rmse = c(lm.pred.rmse, svm.pred.rmse, knn.pred.rmse, nnet.pred.rmse, bayesglm.pred.rmse, rf.pred.rmse),
r2 = c(lm.pred.r2, svm.pred.r2, knn.pred.r2, nnet.pred.r2, bayesglm.pred.r2, rf.pred.r2))
row.names(df) <- c('Linear Model', 'SVM', 'kNN', 'Neural Network', 'Bayes GLM', 'Random Forest')
rmse.plot <- ggplot(df, aes(y=reorder(rownames(df), rmse), x=rmse)) +
geom_point() +
geom_text(label=round(df$rmse, 2), vjust=-1, size=3) +
labs(x='RMSE', y='') +
xlim(0,20)
r2.plot <- ggplot(df, aes(y=reorder(rownames(df), rmse), x=r2)) +
geom_point() +
geom_text(label=round(df$r2, 2), vjust=-1, size=3) +
labs(x='Rsquared', y='') +
xlim(0.7,1)
plots <- arrangeGrob(rmse.plot, r2.plot, ncol=2)
ggsave(plots, file="R\\plots\\model_comp_other.png", width=8, height=3)
# ============================================================================
# RMSE and R^2 plot for different machine learning models
# ============================================================================
# Predict for testing set
lm.pred <- predict(lmFit, test.scaled)
svm.pred <- predict(svmFit, test.scaled)
knn.pred <- predict(knnFit, test.scaled)
nnet.pred <- predict(nnetFit, test.scaled)
bayesglm.pred <- predict(bayesglmFit, test.scaled)
rf.pred <- predict(rfFit, test.scaled)
# R^2 values
lm.pred.r2 <- cor(lm.pred, test.scaled$yield)
svm.pred.r2 <- cor(svm.pred, test.scaled$yield)
knn.pred.r2 <- cor(knn.pred, test.scaled$yield)
nnet.pred.r2 <- cor(nnet.pred, test.scaled$yield)
bayesglm.pred.r2 <- cor(bayesglm.pred, test.scaled$yield)
rf.pred.r2 <- cor(rf.pred, test.scaled$yield)
# RMSE
lm.pred.rmse <- rmse(lm.pred, test.scaled$yield)
svm.pred.rmse <- rmse(svm.pred, test.scaled$yield)
knn.pred.rmse <- rmse(knn.pred, test.scaled$yield)
nnet.pred.rmse <- rmse(nnet.pred, test.scaled$yield)
bayesglm.pred.rmse <- rmse(bayesglm.pred, test.scaled$yield)
rf.pred.rmse <- rmse(rf.pred, test.scaled$yield)
# Plot RMSE and R^2
df <- data.frame(rmse = c(lm.pred.rmse, svm.pred.rmse, knn.pred.rmse, nnet.pred.rmse, bayesglm.pred.rmse, rf.pred.rmse),
r2 = c(lm.pred.r2, svm.pred.r2, knn.pred.r2, nnet.pred.r2, bayesglm.pred.r2, rf.pred.r2))
row.names(df) <- c('Linear Model', 'SVM', 'kNN', 'Neural Network', 'Bayes GLM', 'Random Forest')
rmse.plot <- ggplot(df, aes(y=reorder(rownames(df), rmse), x=rmse)) +
geom_point() +
geom_text(label=round(df$rmse, 2), vjust=-1, size=3) +
labs(x='RMSE', y='') +
xlim(0,20)
r2.plot <- ggplot(df, aes(y=reorder(rownames(df), rmse), x=r2)) +
geom_point() +
geom_text(label=round(df$r2, 2), vjust=-1, size=3) +
labs(x='Rsquared', y='') +
xlim(0.7,1)
plots <- arrangeGrob(rmse.plot, r2.plot, ncol=2)
ggsave(plots, file="R\\plots\\ml_models.png", width=8, height=3)
# ============================================================================
# Calibration plots for different machine learning models
# ============================================================================
# Create data frames with predicted and observed values for test set
df1 <- data.frame(x = knn.pred, y = test.scaled$yield, type = as.factor('kNN'))
df2 <- data.frame(x = svm.pred, y = test.scaled$yield, type = as.factor('SVM'))
df3 <- data.frame(x = bayesglm.pred, y = test.scaled$yield, type = as.factor('Bayes GLM'))
df4 <- data.frame(x = lm.pred, y = test.scaled$yield, type = as.factor('Linear Model'))
df5 <- data.frame(x = nnet.pred, y = test.scaled$yield, type = as.factor('Neural Network'))
df6 <- data.frame(x = rf.pred, y = test.scaled$yield, type = as.factor('Random Forest'))
# Make calibration plots
facet.df <- do.call(rbind, list(df1, df2, df3, df4, df5, df6))
facet.plot <- ggplot(facet.df, aes(x = x, y = y)) +
geom_point(alpha = 0.3, color="dodgerblue3", size=1) +
scale_x_continuous(breaks = seq(-25,100,25), lim=c(-25, 100)) +
geom_segment(aes(x=0, xend=100, y=0, yend=100), linetype="dashed", size=0.3) +
geom_smooth(method="loess", se=FALSE, size=0.5, color="black") +
facet_wrap(~type, ncol=2) +
labs(x='Predicted Yield', y='Observed Yield')
ggsave(file="R\\plots\\ml_calibration_plots.png", width=8, height=9)
# ============================================================================
# Random forest: Predicting out-of-sample additives
# ============================================================================
# double check that row numbers are correct by testing number of unique additives
# length(unique(output.scaled$additive_.C3_NMR_shift[1:1075])) == 6 # TRUE (one more row is 7)
# length(unique(output.scaled$additive_.C3_NMR_shift[1:2515])) == 14 # TRUE (one more row is 15)
# length(unique(output.scaled$additive_.C3_NMR_shift)) == 22
# plates 1 and 2 (after NA's removed)
plate12 <- output.scaled[1:2515, ]
# plate 3 (after NA's removed)
plate3 <- output.scaled[2516:3955, ]
# leave-one-out by additive
by.additive <- split(seq_along(plate12$additive_.C3_NMR_shift), plate12$additive_.C3_NMR_shift)
tc.additive <- trainControl(method="cv", indexOut=by.additive, savePredictions = TRUE)
# leave-one-out random forest (70% of data)
set.seed(8915)
rfFit.LOO <- train(yield ~ ., data=plate12, trControl=tc.additive, method="rf", importance=TRUE)
png(filename="R\\plots\\rf_LOO.png", width = 1000, height = 600)
predVals <- extractPrediction(list(rfFit.LOO))
plotObsVsPred(predVals)
dev.off()
saveRDS(rfFit.LOO, "rds\\rfFit_LOO.rds")
rf.predTrain.LOO <- predict(rfFit.LOO, plate12)
rf.predTrain.LOO.rmse <- rmse(rf.predTrain.LOO, plate12$yield)
rf.predTrain.LOO.r2 <- cor(rf.predTrain.LOO, plate12$yield)
rf.pred.LOO <- predict(rfFit.LOO, plate3)
rf.pred.LOO.rmse <- rmse(rf.pred.LOO, plate3$yield)
rf.pred.LOO.r2 <- cor(rf.pred.LOO, plate3$yield)
plate3$additive_id <- as.factor(plate3$additive_.C3_NMR_shift)
levels(plate3$additive_id) <- c('Additive 16', 'Additive 18', 'Additive 20', 'Additive 21',
'Additive 22', 'Additive 17', 'Additive 19', 'Additive 23')
plate3$additive_id <- sortLvls.fnc(plate3$additive_id, c(1, 6, 2, 7, 3, 4, 5, 8))
plate3$additive_id = factor(plate3$additive_id,levels(plate3$additive_id)[c(1, 6, 2, 7, 3, 4, 5, 8)])
df <- cbind(plate3, rf.pred.LOO)
p <- ggplot(df, aes(x=rf.pred.LOO, y=yield)) +
geom_point(alpha=0.4, aes(col=additive_id), size=1) +
labs(x='Predicted Yield', y='Observed Yield') +
xlim(0, 100) +
ylim(0, 100) +
geom_smooth(method='lm', se=FALSE, color="black", size=0.5) +
facet_wrap(~additive_id, nrow=2, ncol=4) +
geom_segment(aes(x=0, xend=100, y=0, yend=100), linetype="dashed", size=0.3) +
theme(legend.position="none")
ggsave(file="R\\plots\\additive_out_of_sample.png", width=8, height=4.5)
# ============================================================================
# Random forest under sparsity: Training models
# ============================================================================
# Random forest (2.5% of data)
set.seed(8915)
rfFit2.5 <- train(yield ~ ., data=training2.5, trControl=train_control, method="rf")
saveRDS(rfFit2.5, "rds\\rfFit2_5.rds")
# Random forest (5% of data)
set.seed(8915)
rfFit5 <- train(yield ~ ., data=training5, trControl=train_control, method="rf")
saveRDS(rfFit5, "rds\\rfFit5.rds")
# Random forest (10% of data)
set.seed(8915)
rfFit10 <- train(yield ~ ., data=training10, trControl=train_control, method="rf")
saveRDS(rfFit10, "rds\\rfFit10.rds")
# Random forest (20% of data)
set.seed(8915)
rfFit20 <- train(yield ~ ., data=training20, trControl=train_control, method="rf")
saveRDS(rfFit20, "rds\\rfFit20.rds")
# Random forest (30% of data)
set.seed(8915)
rfFit30 <- train(yield ~ ., data=training30, trControl=train_control, method="rf")
saveRDS(rfFit30, "rds\\rfFit30.rds")
# Random forest (40% of data)
set.seed(8915)
rfFit40 <- train(yield ~ ., data=training40, trControl=train_control, method="rf")
saveRDS(rfFit40, "rds\\rfFit40.rds")
# Random forest (50% of data)
set.seed(8915)
rfFit50 <- train(yield ~ ., data=training50, trControl=train_control, method="rf")
saveRDS(rfFit50, "rds\\rfFit50.rds")
# Random forest (60% of data)
set.seed(8915)
rfFit60 <- train(yield ~ ., data=training60, trControl=train_control, method="rf")
saveRDS(rfFit60, "rds\\rfFit60.rds")
# Random forest (70% of data - all training data - rfFit70 is identical to rfFit)
set.seed(8915)
rfFit70 <- train(yield ~ ., data=training.scaled, trControl=train_control, method="rf", importance=TRUE)
saveRDS(rfFit70, "rds\\rfFit70.rds")
# ============================================================================
# Random forest under sparsity: Making calibration plots
# ============================================================================
# Predict for testing set
rf.pred2.5 <- predict(rfFit2.5, test.scaled)
rf.pred5 <- predict(rfFit5, test.scaled)
rf.pred10 <- predict(rfFit10, test.scaled)
rf.pred20 <- predict(rfFit20, test.scaled)
rf.pred30 <- predict(rfFit30, test.scaled)
rf.pred40 <- predict(rfFit40, test.scaled)
rf.pred50 <- predict(rfFit50, test.scaled)
rf.pred60 <- predict(rfFit60, test.scaled)
rf.pred70 <- predict(rfFit70, test.scaled)
# Plot expected vs. observed
# Create data frames
df1 <- data.frame(x = rf.pred2.5, y = test.scaled$yield, type = as.factor('2.5%'))
df2 <- data.frame(x = rf.pred5, y = test.scaled$yield, type = as.factor('5%'))
df3 <- data.frame(x = rf.pred10, y = test.scaled$yield, type = as.factor('10%'))
df4 <- data.frame(x = rf.pred20, y = test.scaled$yield, type = as.factor('20%'))
df5 <- data.frame(x = rf.pred30, y = test.scaled$yield, type = as.factor('30%'))
df6 <- data.frame(x = rf.pred40, y = test.scaled$yield, type = as.factor('40%'))
df7 <- data.frame(x = rf.pred50, y = test.scaled$yield, type = as.factor('50%'))
df8 <- data.frame(x = rf.pred60, y = test.scaled$yield, type = as.factor('60%'))
df9 <- data.frame(x = rf.pred70, y = test.scaled$yield, type = as.factor('70%'))
facet.df <- do.call(rbind, list(df1, df2, df3,
df4, df5, df6,
df7, df8, df9))
facet.plot <- ggplot(facet.df, aes(x = x, y = y)) +
geom_point(alpha = 0.3, color="dodgerblue3", size=1) +
scale_x_continuous(breaks = seq(0,100,25), lim=c(0, 100)) +
geom_segment(aes(x=0, xend=100, y=0, yend=100), linetype="dashed", size=0.3) +
geom_smooth(method="loess", se=FALSE, size=0.5, color="black") +
facet_wrap(~type, ncol=3) +
labs(x='Predicted Yield', y='Observed Yield')
ggsave(file="R\\plots\\rf_sparsity.png", width=8, height=9)
# ============================================================================
# Random forest under sparsity: Plotting R^2 and RMSE
# ============================================================================
# R^2 values
rf.pred2.5.r2 <- cor(rf.pred2.5, test.scaled$yield)
rf.pred5.r2 <- cor(rf.pred5, test.scaled$yield)
rf.pred10.r2 <- cor(rf.pred10, test.scaled$yield)
rf.pred20.r2 <- cor(rf.pred20, test.scaled$yield)
rf.pred30.r2 <- cor(rf.pred30, test.scaled$yield)
rf.pred40.r2 <- cor(rf.pred40, test.scaled$yield)
rf.pred50.r2 <- cor(rf.pred50, test.scaled$yield)
rf.pred60.r2 <- cor(rf.pred60, test.scaled$yield)
rf.pred70.r2 <- cor(rf.pred70, test.scaled$yield)
# RMSE
rf.pred2.5.rmse <- rmse(rf.pred2.5, test.scaled$yield)
rf.pred5.rmse <- rmse(rf.pred5, test.scaled$yield)
rf.pred10.rmse <- rmse(rf.pred10, test.scaled$yield)
rf.pred20.rmse <- rmse(rf.pred20, test.scaled$yield)
rf.pred30.rmse <- rmse(rf.pred30, test.scaled$yield)
rf.pred40.rmse <- rmse(rf.pred40, test.scaled$yield)
rf.pred50.rmse <- rmse(rf.pred50, test.scaled$yield)
rf.pred60.rmse <- rmse(rf.pred60, test.scaled$yield)
rf.pred70.rmse <- rmse(rf.pred70, test.scaled$yield)
# create data frame containing RMSE and R^2 data for sparsity models
df <- data.frame(rmse = c(rf.pred2.5.rmse,
rf.pred5.rmse,
rf.pred10.rmse,
rf.pred20.rmse,
rf.pred30.rmse,
rf.pred50.rmse,
rf.pred70.rmse),
r2 = c(rf.pred2.5.r2,
rf.pred5.r2,
rf.pred10.r2,
rf.pred20.r2,
rf.pred30.r2,
rf.pred50.r2,
rf.pred70.r2))
row.names(df) <- c('2.5%', '5%', '10%', '20%', '30%', '50%', '70%')
# Plot RMSE and R^2 data
rmse.plot <- ggplot(df, aes(y=reorder(rownames(df), rmse), x=rmse)) +
geom_point() +
geom_text(label=round(df$rmse, 2), vjust=-1, size=2.5) +
labs(x='RMSE', y='') +
xlim(0, 20)
r2.plot <- ggplot(df, aes(y=reorder(rownames(df), rmse), x=r2)) +
geom_point() +
geom_text(label=round(df$r2, 2), vjust=-1, size=2.5) +
labs(x='Rsquared', y='') +
xlim(0.7, 1)
plots <- arrangeGrob(r2.plot, rmse.plot, ncol=2)
ggsave(plots, file="R\\plots\\rf_sparsity_r2_rmse.png", width=6, height=3)
# ============================================================================
# Random forest: Plotting variable importance
# ============================================================================
# read in variable importance from trained random forest model
rf_imp <- importance(rfFit70$finalModel)
rf.imp.df <- cbind(as.data.frame(rf_imp), names(rf_imp[, 1]))
colnames(rf.imp.df)[1] <- "IncMSE"
colnames(rf.imp.df)[3] <- "descriptor"
# for descriptor names, replace "_" with " " and "." with "*"
rf.imp.df$descriptor <- gsub("_", " ", rf.imp.df$descriptor)
rf.imp.df$descriptor <- gsub("[.]", "*", rf.imp.df$descriptor)
# capitalize descriptor names
simpleCap <- function(x) {
s <- strsplit(x, " ")[[1]]
paste(toupper(substring(s, 1, 1)), substring(s, 2),
sep="", collapse=" ")
}
rf.imp.df$descriptor <- sapply(rf.imp.df$descriptor, simpleCap)
# plot variable importance (saves to R\plots\rf_variable_importance.png)
p1 <- ggplot(rf.imp.df[rf.imp.df$IncMSE>15, ], aes(x=reorder(descriptor, IncMSE), y=IncMSE)) +
geom_bar(stat="identity") +
scale_y_continuous(labels = comma) +
labs(x="", y="Increase in Mean Squared Error (%)") +
coord_flip()
ggsave(p1, file="R\\plots\\rf_importance.png", width=6, height=4)
# ============================================================================
# Plotting *C3 NMR Shift vs Yield
# ============================================================================
# loess best fit curve
p1 <- ggplot(output.scaled, aes(x = additive_.C3_NMR_shift, y = yield)) +
geom_point(alpha = 0.2, size=1) +
labs(x='Additive *C3 NMR Shift', y='Yield') +
geom_smooth(method="loess", se=FALSE)
ggsave(file="R\\plots\\C3_vs_yield_loess.png", width=5, height=4)
# linear best fit line
p2 <- ggplot(output.scaled, aes(x = additive_.C3_NMR_shift, y = yield)) +
geom_point(alpha = 0.2, size=1) +
labs(x='Additive *C3 NMR Shift', y='Yield') +
geom_smooth(method="lm", se=FALSE)
ggsave(file="R\\plots\\C3_vs_yield_lm.png", width=5, height=4)
# calculate R^2 value between *C3 NMR shift and yield; display in console
C3.yield.cor <- cor(output.scaled$additive_.C3_NMR_shift, output.scaled$yield)
paste0("R^2 value between *C3 NMR Shift and Yield = ", C3.yield.cor)
# ============================================================================
# Random forest: Train and test each aryl halide individually
# ============================================================================
# Train random forest model (ArCl)
set.seed(8915)
rfFit.ArCl <- train(yield ~ ., data=ArCl.training, trControl=train_control, method="rf", importance=TRUE)
saveRDS(rfFit.ArCl, "rds\\rfFit_ArCl.rds")
# Test on ArCl
ClfromCl <- predict(rfFit.ArCl, ArCl.test)
ClfromCl.r2 <- cor(ClfromCl, ArCl.test$yield)
ClfromCl.rmse <- rmse(ClfromCl, ArCl.test$yield)
df1 <- data.frame(x = ClfromCl,
y = ArCl.test$yield)
# Create calibration plot (predict Cl from Cl using random forest model)
p1 <- ggplot(df1, aes(x = x, y = y)) +
geom_point(alpha = 0.4) +
scale_x_continuous(breaks = seq(0, 100, 25), lim=c(0, 100)) +
labs(x='Predicted Yield', y='Observed Yield') +
geom_segment(aes(x=0, xend=100, y=0, yend=100), linetype="dashed") +
geom_smooth(method="loess", se=FALSE)
ggsave(file="R\\plots\\ClfromCl.png", width=5, height=4)
# Train random forest model (ArBr)
set.seed(8915)
rfFit.ArBr <- train(yield ~ ., data=ArBr.training, trControl=train_control, method="rf", importance=TRUE)
saveRDS(rfFit.ArBr, "rds\\rfFit_ArBr.rds")
# Test on ArBr
BrfromBr <- predict(rfFit.ArBr, ArBr.test)
BrfromBr.r2 <- cor(BrfromBr, ArBr.test$yield)
BrfromBr.rmse <- rmse(BrfromBr, ArBr.test$yield)
df2 <- data.frame(x = BrfromBr,
y = ArBr.test$yield)
# Create calibration plot (predict Br from Br using random forest model)
p2 <- ggplot(df2, aes(x = x, y = y)) +
geom_point(alpha = 0.4) +
scale_x_continuous(breaks = seq(0,100,25), lim=c(0, 100)) +
labs(x='Predicted Yield', y='Observed Yield') +
geom_segment(aes(x=0, xend=100, y=0, yend=100), linetype="dashed") +
geom_smooth(method="loess", se=FALSE)
ggsave(file="R\\plots\\BrfromBr.png", width=5, height=4)
# Train random forest model (ArI)
set.seed(8915)
rfFit.ArI <- train(yield ~ ., data=ArI.training, trControl=train_control, method="rf", importance=TRUE)
saveRDS(rfFit.ArI, "rds\\rfFit_ArI.rds")
# Test on ArI
IfromI <- predict(rfFit.ArI, ArI.test)
IfromI.r2 <- cor(IfromI, ArI.test$yield)
IfromI.rmse <- rmse(IfromI, ArI.test$yield)
df3 <- data.frame(x = IfromI,
y = ArI.test$yield)
# Create calibration plot (predict Br from Br using random forest model)
p3 <- ggplot(df3, aes(x = x, y = y)) +
geom_point(alpha = 0.4) +
scale_x_continuous(breaks = seq(0,100,25), lim=c(0, 100)) +
labs(x='Predicted Yield', y='Observed Yield') +
geom_segment(aes(x=0, xend=100, y=0, yend=100), linetype="dashed") +
geom_smooth(method="loess", se=FALSE)
ggsave(file="R\\plots\\IfromI.png", width=5, height=4)
# Calculate R^2 and RMSE
rf.pred.ArCl <- predict(rfFit.ArCl, ArCl.test)
rf.pred.ArCl.r2 <- cor(rf.pred.ArCl, ArCl.test$yield)
rf.pred.ArCl.rmse <- rmse(rf.pred.ArCl, ArCl.test$yield)
rf.pred.ArBr <- predict(rfFit.ArBr, ArBr.test)
rf.pred.ArBr.r2 <- cor(rf.pred.ArBr, ArBr.test$yield)
rf.pred.ArBr.rmse <- rmse(rf.pred.ArBr, ArBr.test$yield)
rf.pred.ArI <- predict(rfFit.ArI, ArI.test)
rf.pred.ArI.r2 <- cor(rf.pred.ArI, ArI.test$yield)
rf.pred.ArI.rmse <- rmse(rf.pred.ArI, ArI.test$yield)
# Pretty plot RMSE and Rsquared
df <- data.frame(rmse = c(rf.pred.ArCl.rmse, rf.pred.ArBr.rmse, rf.pred.ArI.rmse),
r2 = c(rf.pred.ArCl.r2, rf.pred.ArBr.r2, rf.pred.ArCl.r2))
row.names(df) <- c('ArCl', 'ArBr', 'ArI')
rmse.plot <- ggplot(df, aes(y=rownames(df), x=rmse)) +
geom_point() +
geom_text(label=round(df$rmse, 2), vjust=-1, size=3) +
labs(x='RMSE', y='') +
xlim(0,20)
r2.plot <- ggplot(df, aes(y=rownames(df), x=r2)) +
geom_point() +
geom_text(label=round(df$r2, 2), vjust=-1, size=3) +
labs(x='Rsquared', y='') +
xlim(0.7,1)
plots <- arrangeGrob(rmse.plot, r2.plot, ncol=2)
ggsave(plots, file="R\\plots\\rf_aryl_halides.png", width=6, height=2)
# ============================================================================
# Random forest: Predict ArCl and ArI from ArBr
# ============================================================================
# Random forest (ArBr)
set.seed(8915)
rfFit.ArBr.all <- train(yield ~ ., data=ArBr.scaled, trControl=train_control, method="rf", importance=TRUE)
saveRDS(rfFit.ArBr.all, "rds\\rfFit_ArBr_all.rds")
# Test on ArCl
ClfromBr <- predict(rfFit.ArBr.all, ArCl.scaled)
ClfromBr.r2 <- cor(ClfromBr, ArCl.scaled$yield)
ClfromBr.rmse <- rmse(ClfromBr, ArCl.scaled$yield)
df1 <- data.frame(x = ClfromBr,
y = ArCl.scaled$yield)
# Create calibration plot (predict Cl from Br random forest model)
p1 <- ggplot(df1, aes(x = x, y = y)) +
geom_point(alpha = 0.4) +
scale_x_continuous(breaks = seq(0,100,25), lim=c(0, 100)) +
labs(x='Predicted Yield', y='Observed Yield') +
geom_segment(aes(x=0, xend=100, y=0, yend=100), linetype="dashed") +
geom_smooth(method="loess", se=FALSE)
ggsave(file="R\\plots\\ClfromBr.png", width=5, height=4)
# Test on ArI
IfromBr <- predict(rfFit.ArBr.all, ArI.scaled)
IfromBr.r2 <- cor(IfromBr, ArI.scaled$yield)
IfromBr.rmse <- rmse(IfromBr, ArI.scaled$yield)
df2 <- data.frame(x = IfromBr,
y = ArI.scaled$yield)
# Create calibration plot (predict I from Br random forest model)
p2 <- ggplot(df2, aes(x = x, y = y)) +
geom_point(alpha = 0.4) +
scale_x_continuous(breaks = seq(0,100,25), lim=c(0, 100)) +
labs(x='Predicted Yield', y='Observed Yield') +
geom_segment(aes(x=0, xend=100, y=0, yend=100), linetype="dashed") +
geom_smooth(method="loess", se=FALSE)
ggsave(file="R\\plots\\IfromBr.png", width=5, height=4)
# ============================================================================
# Random forest: Predicting pyridyl substrates from nonpyridyl ones
# ============================================================================
# Random forest (train on nonpyridyl)
set.seed(8915)
rfFit.nonpyridyl <- train(yield ~ ., data=nonpyridyl.scaled, trControl=train_control, method="rf", importance=TRUE)
png(filename="R\\plots\\rf_nonpyridyl.png", width = 1000, height = 600)
predVals <- extractPrediction(list(rfFit.nonpyridyl))
plotObsVsPred(predVals)
dev.off()
saveRDS(rfFit.nonpyridyl, "rds\\rfFit_nonpyridyl.rds")
# Test on pyridyl
pyridyl.pred <- predict(rfFit.nonpyridyl, pyridyl.scaled)
pyridyl.r2 <- cor(pyridyl.pred, pyridyl.scaled$yield)
pyridyl.rmse <- rmse(pyridyl.pred, pyridyl.scaled$yield)
df1 <- data.frame(x = pyridyl.pred,
y = pyridyl.scaled$yield)
p1 <- ggplot(df1, aes(x = x, y = y)) +
geom_point(alpha = 0.4) +
scale_x_continuous(breaks = seq(0,100,25), lim=c(0, 100)) +
labs(x='Predicted Yield', y='Observed Yield') +
geom_segment(aes(x=0, xend=100, y=0, yend=100), linetype="dashed") +
geom_smooth(method="loess", se=FALSE)
ggsave(file="R\\plots\\pyr_from_nonpyr.png", width=5, height=4)
# ============================================================================
# Random forest: Predicting yields > 80% from yields < 80%
# ============================================================================
# Train random forest model using yields < 80%
set.seed(8915)
rfFit.under80 <- train(yield ~ ., data=output.under80, trControl=train_control, method="rf", importance=TRUE)
saveRDS(rfFit.under80, "rds\\rfFit_under80.rds")
# Predict yields for reactions > 80%
over80pred <- predict(rfFit.under80, output.over80)
over80pred.r2 <- cor(over80pred, output.over80$yield)
over80pred.rmse <- rmse(over80pred, output.over80$yield)
# Generate calibration plot
df <- data.frame(x = over80pred,
y = output.over80$yield)
p1 <- ggplot(df, aes(x = x, y = y)) +
geom_point(alpha = 0.4) +
scale_x_continuous(breaks = seq(0, 100, 25), lim=c(0, 100)) +
labs(x='Predicted Yield', y='Observed Yield') +
geom_segment(aes(x=0, xend=100, y=0, yend=100), linetype="dashed") +
geom_smooth(method="loess", se=FALSE)
ggsave(file="R\\plots\\over80pred.png", width=5, height=4)
